{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a46c854-c99b-4b52-8748-6146036b5a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57975777-272a-4d07-a15d-ce96062b65e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (2.6.0.dev20241030)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (0.18.1a0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (2.5.0.dev20241030)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ML1/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cea717-27aa-4755-8c26-17a73b5224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "# Libraries to import the dataset into the code and split it\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for PyTorch model building and data processing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import time\n",
    "\n",
    "# Libraries for tokenization\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "\n",
    "# Evaluation metric\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cb3f7c-df1e-4605-a133-a15eee416c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Available: True\n"
     ]
    }
   ],
   "source": [
    "# checking if my Mac can leverage Apple’s Metal Performance Shaders (MPS)\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494c92a8-3396-4050-a01d-213d32b8ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS is available and set the device\n",
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfc20dd-4d09-4125-be2a-f0ff3e798dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to clean text\n",
    "def preprocess_text(text):\n",
    "    #Preprocess text by removing special characters.\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text.strip()  # Only stripping extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf2639d-7c52-4c43-ab4b-60ecacefca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_lang8_data(file_path):\n",
    "    #Loads and preprocesses the dataset from a CSV file.\n",
    "    #Assumes the CSV file has two columns: '0' (original) and '1' (corrected).\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "    inputs = data['0'].apply(preprocess_text).tolist()  # Original text\n",
    "    targets = data['1'].apply(preprocess_text).tolist()  # Corrected text\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bb6721-5040-42f4-8f94-4e9bbdc45c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "inputs, targets = load_lang8_data(\"Cleaned_Lang8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31cb83c2-1819-48fe-a398-5a39bae338b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the president was standing in the front row and the every female enployees were surrounding him'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716eb6ae-f4b5-43f4-ba48-b81a5932b8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the president was standing in the front row and all the female employees were surrounding him'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc32b64d-0ae4-4506-b985-7d055fe0808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets with shuffling (80% train, 20% validation)\n",
    "train_inputs, valid_inputs, train_targets, valid_targets = train_test_split(\n",
    "    inputs[:10000], targets[:10000], test_size=0.2, random_state=9, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1e4e20-30ec-4ba5-a15b-eaea703cbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a684bd0-7ffc-4cd8-a780-03fd4d874d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "def tokenize_data(sentences):\n",
    "    #Tokenizes sentences using the BERT tokenizer and returns the tokenized tensors.\n",
    "    \n",
    "    return tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aabfec5-f473-4e17-a637-4b1495b2da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and validation data directly\n",
    "train_encodings = tokenize_data(train_inputs)\n",
    "train_labels = tokenize_data(train_targets)\n",
    "valid_encodings = tokenize_data(valid_inputs)\n",
    "valid_labels = tokenize_data(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c9f5ac-467e-4373-9cfe-3ffc8842fd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1165,   178,  ...,     0,     0,     0],\n",
       "        [  101, 13280,  5171,  ...,     0,     0,     0],\n",
       "        [  101,  1177,  1208,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  7455,  1106,  ...,     0,     0,     0],\n",
       "        [  101,   178,  2023,  ...,     0,     0,     0],\n",
       "        [  101,  1196,  1374,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d51db29-8e7d-4266-ab74-ccd0de78ac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1165,  178,  ...,    0,    0,    0],\n",
       "        [ 101,  178, 1156,  ...,    0,    0,    0],\n",
       "        [ 101, 1177, 1208,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 7455, 1106,  ...,    0,    0,    0],\n",
       "        [ 101,  178, 1108,  ...,    0,    0,    0],\n",
       "        [ 101, 1107,  170,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f18cb4-a595-443c-b8b4-888c1c7d4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Custom Sequence-to-Sequence Model (RNN-based Encoder-Decoder)\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding(src)\n",
    "        encoder_output, (hidden, cell) = self.encoder(embedded_src)\n",
    "        \n",
    "        embedded_tgt = self.embedding(tgt)\n",
    "        decoder_output, _ = self.decoder(embedded_tgt, (hidden, cell))\n",
    "        \n",
    "        output = self.fc(decoder_output)\n",
    "        return output\n",
    "\n",
    "seq2seq_model = Seq2SeqModel(vocab_size=len(tokenizer.vocab), embed_size=256, hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285505a6-f16c-4956-99b8-4933dbcb91e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML1/lib/python3.9/site-packages/torch/nn/modules/transformer.py:375: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Custom Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, ff_hidden_size, max_len, device=\"cpu\"):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device  # Store device to move tensors accordingly\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size).to(self.device)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_len, embed_size).to(self.device)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_size, num_heads, ff_hidden_size)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers).to(self.device)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(embed_size, num_heads, ff_hidden_size)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers).to(self.device)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_size, vocab_size).to(self.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Ensure input tensors are on the same device\n",
    "        src = src.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        \n",
    "        src = self.embedding(src) + self.positional_encoding[:src.size(1), :].to(self.device)\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:tgt.size(1), :].to(self.device)\n",
    "        \n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        \n",
    "        return self.fc(output)\n",
    "\n",
    "    def get_positional_encoding(self, max_len, embed_size):\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i] = np.sin(pos / (10000 ** (i / embed_size)))\n",
    "                pe[pos, i + 1] = np.cos(pos / (10000 ** (i / embed_size)))\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "transformer_model = TransformerModel(\n",
    "    vocab_size=len(tokenizer.vocab), embed_size=128, num_heads=4, num_layers=2, ff_hidden_size=512, max_len=512, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9ad9433-374e-41a1-8d63-7fb14e8ef0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_encodings, train_labels, epochs=1, batch_size=64):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    total_batches = (len(train_encodings['input_ids']) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(range(total_batches), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in progress_bar:\n",
    "            # Create batches manually\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(train_encodings['input_ids']))\n",
    "            \n",
    "            # Extract the batch and move tensors to MPS\n",
    "            input_ids = train_encodings['input_ids'][start_idx:end_idx].to(device)\n",
    "            labels = train_labels['input_ids'][start_idx:end_idx].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, input_ids)  # Move all batch items to device\n",
    "\n",
    "            # Ensure outputs and labels are on the same device\n",
    "            outputs = outputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        average_epoch_loss = epoch_loss / total_batches\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} completed with Average Loss: {average_epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2fab8d-3aa5-4dd1-a3d7-43d2512571e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating function\n",
    "def evaluate_model(model, valid_encodings, valid_labels, tokenizer, batch_size=64):\n",
    "    \n",
    "    # Evaluates the model using BLEU score and prints the average BLEU score.\n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Move encodings to device\n",
    "    input_ids = valid_encodings[\"input_ids\"].to(device)\n",
    "    labels = valid_labels[\"input_ids\"].to(device)\n",
    "\n",
    "    # Evaluate in batches\n",
    "    num_batches = (input_ids.size(0) + batch_size - 1) // batch_size\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_batches), desc=\"Evaluating\"):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, input_ids.size(0))\n",
    "\n",
    "            # Extract batch\n",
    "            input_batch = input_ids[start_idx:end_idx]\n",
    "            label_batch = labels[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch, input_batch)  # Both src and tgt are input_ids for simplicity\n",
    "            \n",
    "            # Move predictions to CPU for BLEU calculation\n",
    "            predictions = outputs.argmax(dim=-1).cpu()\n",
    "            label_batch = label_batch.cpu()\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            output_texts = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "            label_texts = [tokenizer.decode(label, skip_special_tokens=True) for label in label_batch]\n",
    "\n",
    "            # Calculate BLEU score for each sentence\n",
    "            batch_scores = [\n",
    "                sentence_bleu([label.split()], output.split())\n",
    "                for label, output in zip(label_texts, output_texts)\n",
    "            ]\n",
    "            scores.extend(batch_scores)\n",
    "\n",
    "    # Calculate the average BLEU score\n",
    "    average_bleu = np.mean(scores)\n",
    "    print(\"Average BLEU Score:\", average_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc5112f-9847-4f2f-ad69-6f150ccc68b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (embedding): Embedding(28996, 256)\n",
       "  (encoder): LSTM(256, 512, batch_first=True)\n",
       "  (decoder): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the device\n",
    "seq2seq_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36c03d2d-777a-47e0-a250-de304ef8f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████| 125/125 [09:03<00:00,  4.35s/batch, batch_loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed with Average Loss: 0.4555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████| 125/125 [09:05<00:00,  4.37s/batch, batch_loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed with Average Loss: 0.1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████| 125/125 [09:01<00:00,  4.33s/batch, batch_loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed with Average Loss: 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting timer to measure time it takes to train the model\n",
    "mod1train_starttime = time.time()\n",
    "\n",
    "# Train the sequence-to-sequence model\n",
    "train_model(seq2seq_model, train_encodings, train_labels, epochs=3)\n",
    "\n",
    "# Ending timer to measure time it takes to train the model\n",
    "mod1train_endtime = time.time()\n",
    "\n",
    "# Measuring the time\n",
    "mod1train_time = mod1train_endtime - mod1train_starttime\n",
    "mod1train_hours = mod1train_time / 3600\n",
    "mod1train_minutes = (mod1train_time % 3600) / 60\n",
    "mod1train_seconds = mod1train_time % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12a94bc-6b3e-4926-bf32-d76c1102c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took: 0 hours, 27 minutes, and 10 seconds\n"
     ]
    }
   ],
   "source": [
    "print (f\"Time it took: {int(mod1train_hours)} hours, {int(mod1train_minutes)} minutes, and {int(mod1train_seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b202ddb4-4e28-423d-8f2c-eba6e97cc8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]/opt/anaconda3/envs/ML1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/envs/ML1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/envs/ML1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:59<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.07982769435203343\n",
      "Time it took: 0 hours, 0 minutes, and 59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting timer to measure time it takes to evaluate the model\n",
    "mod1eval_starttime = time.time()\n",
    "\n",
    "# Evaluate the sequence-to-sequence model\n",
    "evaluate_model(seq2seq_model, valid_encodings, valid_labels, tokenizer)\n",
    "\n",
    "# Ending timer to measure time it takes to evaluate the model\n",
    "mod1eval_endtime = time.time()\n",
    "\n",
    "# Measuring the time\n",
    "mod1eval_time = mod1eval_endtime - mod1eval_starttime\n",
    "mod1eval_hours = mod1eval_time / 3600\n",
    "mod1eval_minutes = (mod1eval_time % 3600) / 60\n",
    "mod1eval_seconds = mod1eval_time % 60\n",
    "\n",
    "print (f\"Time it took: {int(mod1eval_hours)} hours, {int(mod1eval_minutes)} minutes, and {int(mod1eval_seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17c7ae3a-5d93-4005-9113-2ee1690ad02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq2seq_model.state_dict(), \"seq2seq_model_311024.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0803e357-0ace-4605-bb5c-03c00ae92a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding): Embedding(28996, 128)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the device\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26d157e2-4ede-428e-8a20-2de574bec12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████| 125/125 [06:42<00:00,  3.22s/batch, batch_loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed with Average Loss: 0.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████| 125/125 [06:44<00:00,  3.24s/batch, batch_loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed with Average Loss: 0.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████| 125/125 [06:37<00:00,  3.18s/batch, batch_loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed with Average Loss: 0.1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting timer to measure time it takes to train the model\n",
    "mod2train_starttime = time.time()\n",
    "\n",
    "# Train the transformer model\n",
    "train_model(transformer_model, train_encodings, train_labels, epochs=3)\n",
    "\n",
    "# Ending timer to measure time it takes to train the model\n",
    "mod2train_endtime = time.time()\n",
    "\n",
    "# Measuring the time\n",
    "mod2train_time = mod2train_endtime - mod2train_starttime\n",
    "mod2train_hours = mod2train_time / 3600\n",
    "mod2train_minutes = (mod2train_time % 3600) / 60\n",
    "mod2train_seconds = mod2train_time % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c2a078-fd0e-4a5f-96ed-6f3720535ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took: 0 hours, 20 minutes, and 5 seconds\n"
     ]
    }
   ],
   "source": [
    "print (f\"Time it took: {int(mod2train_hours)} hours, {int(mod2train_minutes)} minutes, and {int(mod2train_seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc20a5f2-3118-46e5-ba6f-c5987a8769fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:31<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.1285129634991028\n",
      "Time it took: 0 hours, 0 minutes, and 31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting timer to measure time it takes to evaluate the model\n",
    "mod2eval_starttime = time.time()\n",
    "\n",
    "# Evaluate the transformer model\n",
    "evaluate_model(transformer_model, valid_encodings, valid_labels, tokenizer)\n",
    "\n",
    "# Ending timer to measure time it takes to evaluate the model\n",
    "mod2eval_endtime = time.time()\n",
    "\n",
    "# Measuring the time\n",
    "mod2eval_time = mod2eval_endtime - mod2eval_starttime\n",
    "mod2eval_hours = mod2eval_time / 3600\n",
    "mod2eval_minutes = (mod2eval_time % 3600) / 60\n",
    "mod2eval_seconds = mod2eval_time % 60\n",
    "\n",
    "print (f\"Time it took: {int(mod2eval_hours)} hours, {int(mod2eval_minutes)} minutes, and {int(mod2eval_seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1365a10b-92e5-405a-981b-615fbc35c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer_model.state_dict(), \"transformer_model_311024.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23acf796-07f4-44f5-be2d-d2f5603cba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a text for prediction by tokenizing it and moving it into the MPS\n",
    "def prepare_text(text, tokenizer, device):\n",
    "    # Tokenize and convert to tensor\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return encoding['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df2ee3d7-3ca5-4360-b8ff-bcceaa81b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq Correction: the is a a the with with with with is you today do you do do do\n",
      "Transformer Correction: is a i with a to and to is you not do you do yesterday the\n"
     ]
    }
   ],
   "source": [
    "# generate the predicition of corrected text using the original\n",
    "def generate_prediction(text, model, tokenizer, device=\"cpu\"):\n",
    "\n",
    "    # Prepare the input text\n",
    "    input_ids = prepare_text(text, tokenizer, device)\n",
    "\n",
    "    # Pass input through model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, input_ids)  # Using input_ids as both source and target\n",
    "        predictions = outputs.argmax(dim=-1)\n",
    "\n",
    "    # Decode predictions to text\n",
    "    corrected_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "    return corrected_text\n",
    "\n",
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#seq2seq_model.to(device)\n",
    "#transformer_model.to(device)\n",
    "\n",
    "text = \"These is a example sentence with bad grammar. How is you today? What do you do yesterday?\"\n",
    "corrected_seq2seq = generate_prediction(text, seq2seq_model, tokenizer, device)\n",
    "corrected_transformer = generate_prediction(text, transformer_model, tokenizer, device)\n",
    "\n",
    "print(\"Seq2Seq Correction:\", corrected_seq2seq)\n",
    "print(\"Transformer Correction:\", corrected_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ed3c9-e731-4aa9-a1ff-82b62bc7607f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
